diff --git a/skyplane/cli/cli.py b/skyplane/cli/cli.py
index dc7d9ff..867ae9d 100644
--- a/skyplane/cli/cli.py
+++ b/skyplane/cli/cli.py
@@ -158,16 +158,12 @@ def cp(
                 client = ObjectStoreInterface.create(src_region_tag, bucket_src)
                 src_region_tag = client.region_tag()
                 size_byte = get_usage_gbits(dst)
-            throughput_gbps = size_byte / 2**30 / request_time
+            throughput_gbps = size_byte / 2 ** 30 / request_time
 
             # print stats
             if not rc:
                 print_stats_completed(request_time, throughput_gbps)
-                transfer_stats = TransferStats(
-                    monitor_status="completed",
-                    total_runtime_s=request_time,
-                    throughput_gbits=throughput_gbps,
-                )
+                transfer_stats = TransferStats(monitor_status="completed", total_runtime_s=request_time, throughput_gbits=throughput_gbps)
                 UsageClient.log_transfer(transfer_stats, args, src_region_tag, dst_region_tag)
             return 0
         else:
diff --git a/skyplane/cli/common.py b/skyplane/cli/common.py
index 51343f7..5cba3f4 100644
--- a/skyplane/cli/common.py
+++ b/skyplane/cli/common.py
@@ -27,7 +27,7 @@ def print_header():
  `--. \    \   \ / |  __/| |    |  _  || . ` ||  __| 
 /\__/ / |\  \  | | | |   | |____| | | || |\  || |___ 
 \____/\_| \_/  \_/ \_|   \_____/\_| |_/\_| \_/\____/"""
-    console.print(f"[blue]{header}[/blue]\n")
+    console.print(f"[bright_black]{header}[/bright_black]\n")
 
 
 def parse_path(path: str):
diff --git a/skyplane/cli/experiments/cli_query.py b/skyplane/cli/experiments/cli_query.py
index 4d17070..a8173be 100644
--- a/skyplane/cli/experiments/cli_query.py
+++ b/skyplane/cli/experiments/cli_query.py
@@ -14,7 +14,7 @@ def util_grid_throughput(
     throughput_grid: Path = typer.Option(skyplane_root / "profiles" / "throughput.csv", help="Throughput grid file"),
 ):
     solver = ThroughputSolver(throughput_grid)
-    print(solver.get_path_throughput(src, dest, src_tier, dest_tier) / 2**30)
+    print(solver.get_path_throughput(src, dest, src_tier, dest_tier) / 2 ** 30)
 
 
 def util_grid_cost(
diff --git a/skyplane/cli/usage/client.py b/skyplane/cli/usage/client.py
index d515bb3..1a7c2a2 100644
--- a/skyplane/cli/usage/client.py
+++ b/skyplane/cli/usage/client.py
@@ -281,12 +281,7 @@ class UsageClient:
         data.sent_time = int(time.time() * 1000)
         payload = {"streams": [{"stream": prom_labels, "values": [[str(_get_current_timestamp_ns()), json.dumps(asdict(data))]]}]}
         payload = json.dumps(payload)
-        r = requests.post(
-            skyplane.cli.usage.definitions.LOKI_URL,
-            headers=headers,
-            data=payload,
-            timeout=0.5,
-        )
+        r = requests.post(skyplane.cli.usage.definitions.LOKI_URL, headers=headers, data=payload, timeout=0.5)
 
         if r.status_code == 204:
             with open(path, "w") as json_file:
diff --git a/skyplane/compute/aws/aws_cloud_provider.py b/skyplane/compute/aws/aws_cloud_provider.py
index 633015d..2e4fbd7 100644
--- a/skyplane/compute/aws/aws_cloud_provider.py
+++ b/skyplane/compute/aws/aws_cloud_provider.py
@@ -306,20 +306,20 @@ class AWSCloudProvider(CloudProvider):
         key_name = f"skyplane-{aws_region}"
         local_key_file = prefix / f"{key_name}.pem"
 
+        local_key_file.parent.mkdir(parents=True, exist_ok=True)
         if not local_key_file.exists():
-            if not local_key_file.exists():  # double check due to lock
-                local_key_file.parent.mkdir(parents=True, exist_ok=True)
-                if key_name in set(p["KeyName"] for p in ec2_client.describe_key_pairs()["KeyPairs"]):
-                    logger.fs.warning(f"Deleting key {key_name} in region {aws_region}")
-                    ec2_client.delete_key_pair(KeyName=key_name)
-                key_pair = ec2.create_key_pair(KeyName=f"skyplane-{aws_region}", KeyType="rsa")
-                with local_key_file.open("w") as f:
-                    key_str = key_pair.key_material
-                    if not key_str.endswith("\n"):
-                        key_str += "\n"
-                    f.write(key_str)
-                os.chmod(local_key_file, 0o600)
-                logger.fs.info(f"Created key file {local_key_file}")
+            logger.fs.debug(f"[AWS] Creating keypair {key_name} in {aws_region}")
+            if key_name in set(p["KeyName"] for p in ec2_client.describe_key_pairs()["KeyPairs"]):
+                logger.fs.warning(f"Deleting key {key_name} in region {aws_region}")
+                ec2_client.delete_key_pair(KeyName=key_name)
+            key_pair = ec2.create_key_pair(KeyName=f"skyplane-{aws_region}", KeyType="rsa")
+            with local_key_file.open("w") as f:
+                key_str = key_pair.key_material
+                if not key_str.endswith("\n"):
+                    key_str += "\n"
+                f.write(key_str)
+            os.chmod(local_key_file, 0o600)
+            logger.fs.info(f"Created key file {local_key_file}")
 
         return local_key_file
 
diff --git a/skyplane/compute/aws/aws_server.py b/skyplane/compute/aws/aws_server.py
index 23904e2..4037e3b 100644
--- a/skyplane/compute/aws/aws_server.py
+++ b/skyplane/compute/aws/aws_server.py
@@ -5,6 +5,7 @@ import warnings
 from cryptography.utils import CryptographyDeprecationWarning
 
 from skyplane.utils import imports
+from skyplane import exceptions
 
 with warnings.catch_warnings():
     warnings.filterwarnings("ignore", category=CryptographyDeprecationWarning)
@@ -92,16 +93,21 @@ class AWSServer(Server):
     def get_ssh_client_impl(self):
         client = paramiko.SSHClient()
         client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
-        client.connect(
-            self.public_ip(),
-            username="ec2-user",
-            # todo generate keys with password "skyplane"
-            pkey=paramiko.RSAKey.from_private_key_file(str(self.local_keyfile)),
-            look_for_keys=False,
-            allow_agent=False,
-            banner_timeout=200,
-        )
-        return client
+        try:
+            client.connect(
+                self.public_ip(),
+                username="ec2-user",
+                # todo generate keys with password "skyplane"
+                pkey=paramiko.RSAKey.from_private_key_file(str(self.local_keyfile)),
+                look_for_keys=False,
+                allow_agent=False,
+                banner_timeout=200,
+            )
+            return client
+        except paramiko.AuthenticationException as e:
+            raise exceptions.BadConfigException(
+                f"Failed to connect to AWS server {self.uuid()}. Delete local AWS keys and retry: `rm -rf {key_root / 'aws'}`"
+            ) from e
 
     def get_sftp_client(self):
         t = paramiko.Transport((self.public_ip(), 22))
diff --git a/skyplane/compute/azure/azure_server.py b/skyplane/compute/azure/azure_server.py
index 9246907..80d273b 100644
--- a/skyplane/compute/azure/azure_server.py
+++ b/skyplane/compute/azure/azure_server.py
@@ -9,6 +9,7 @@ with warnings.catch_warnings():
     import paramiko
 
 from skyplane import key_root
+from skyplane import exceptions
 from skyplane.compute.azure.azure_auth import AzureAuthentication
 from skyplane.compute.server import Server, ServerState
 from skyplane.utils.cache import ignore_lru_cache
@@ -168,14 +169,19 @@ class AzureServer(Server):
         """Return paramiko client that connects to this instance."""
         ssh_client = paramiko.SSHClient()
         ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
-        ssh_client.connect(
-            hostname=self.public_ip(),
-            username=uname,
-            key_filename=str(self.ssh_private_key),
-            passphrase=ssh_key_password,
-            look_for_keys=False,
-            banner_timeout=200,
-        )
+        try:
+            ssh_client.connect(
+                hostname=self.public_ip(),
+                username=uname,
+                key_filename=str(self.ssh_private_key),
+                passphrase=ssh_key_password,
+                look_for_keys=False,
+                banner_timeout=200,
+            )
+        except paramiko.AuthenticationException as e:
+            raise exceptions.BadConfigException(
+                f"Failed to connect to AWS server {self.uuid()}. Delete local AWS keys and retry: `rm -rf {key_root / 'aws'}`"
+            ) from e
         return ssh_client
 
     def get_sftp_client(self, uname="skyplane", ssh_key_password="skyplane"):
diff --git a/skyplane/compute/gcp/gcp_server.py b/skyplane/compute/gcp/gcp_server.py
index e4b0a56..302dd7e 100644
--- a/skyplane/compute/gcp/gcp_server.py
+++ b/skyplane/compute/gcp/gcp_server.py
@@ -9,6 +9,7 @@ with warnings.catch_warnings():
     import paramiko
 
 from skyplane import key_root
+from skyplane import exceptions
 from skyplane.compute.gcp.gcp_auth import GCPAuthentication
 from skyplane.compute.server import Server, ServerState
 from skyplane.utils.fn import PathLike
@@ -84,13 +85,18 @@ class GCPServer(Server):
         """Return paramiko client that connects to this instance."""
         ssh_client = paramiko.SSHClient()
         ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
-        ssh_client.connect(
-            hostname=self.public_ip(),
-            username=uname,
-            pkey=paramiko.RSAKey.from_private_key_file(str(self.ssh_private_key), password=ssh_key_password),
-            look_for_keys=False,
-            banner_timeout=200,
-        )
+        try:
+            ssh_client.connect(
+                hostname=self.public_ip(),
+                username=uname,
+                pkey=paramiko.RSAKey.from_private_key_file(str(self.ssh_private_key), password=ssh_key_password),
+                look_for_keys=False,
+                banner_timeout=200,
+            )
+        except paramiko.AuthenticationException as e:
+            raise exceptions.BadConfigException(
+                f"Failed to connect to AWS server {self.uuid()}. Delete local AWS keys and retry: `rm -rf {key_root / 'aws'}`"
+            ) from e
         return ssh_client
 
     def get_sftp_client(self, uname="skyplane", ssh_key_password="skyplane"):
diff --git a/skyplane/config.py b/skyplane/config.py
index a8258ab..f6d517a 100644
--- a/skyplane/config.py
+++ b/skyplane/config.py
@@ -90,12 +90,7 @@ class SkyplaneConfig:
 
     @staticmethod
     def default_config() -> "SkyplaneConfig":
-        return SkyplaneConfig(
-            aws_enabled=False,
-            azure_enabled=False,
-            gcp_enabled=False,
-            anon_clientid=None,
-        )
+        return SkyplaneConfig(aws_enabled=False, azure_enabled=False, gcp_enabled=False, anon_clientid=None)
 
     @staticmethod
     def load_config(path) -> "SkyplaneConfig":
diff --git a/skyplane/obj_store/azure_blob_interface.py b/skyplane/obj_store/azure_blob_interface.py
index 5982cb4..34ee485 100644
--- a/skyplane/obj_store/azure_blob_interface.py
+++ b/skyplane/obj_store/azure_blob_interface.py
@@ -116,10 +116,7 @@ class AzureBlobInterface(ObjectStoreInterface):
     ) -> Optional[bytes]:
         src_object_name, dst_file_path = str(src_object_name), str(dst_file_path)
         downloader = self.container_client.download_blob(
-            src_object_name,
-            offset=offset_bytes,
-            length=size_bytes,
-            max_concurrency=self.max_concurrency,
+            src_object_name, offset=offset_bytes, length=size_bytes, max_concurrency=self.max_concurrency
         )
 
         if not os.path.exists(dst_file_path):
@@ -143,11 +140,7 @@ class AzureBlobInterface(ObjectStoreInterface):
         with open(src_file_path, "rb") as f:
             print(f"Uploading {src_file_path} to {dst_object_name}")
             blob_client = self.container_client.upload_blob(
-                name=dst_object_name,
-                data=f,
-                length=os.path.getsize(src_file_path),
-                max_concurrency=self.max_concurrency,
-                overwrite=True,
+                name=dst_object_name, data=f, length=os.path.getsize(src_file_path), max_concurrency=self.max_concurrency, overwrite=True
             )
         if check_md5:
             b64_md5sum = base64.b64encode(check_md5).decode("utf-8") if check_md5 else None
diff --git a/skyplane/obj_store/gcs_interface.py b/skyplane/obj_store/gcs_interface.py
index 6939336..03683cb 100644
--- a/skyplane/obj_store/gcs_interface.py
+++ b/skyplane/obj_store/gcs_interface.py
@@ -218,12 +218,14 @@ class GCSInterface(ObjectStoreInterface):
                 f"Upload of object {dst_object_name} in bucket {self.bucket_name} failed, got status code {response.status_code} w/ response {response.text}"
             )
 
-    def initiate_multipart_upload(self, dst_object_name):
-        assert len(dst_object_name) > 0, f"Destination object name must be non-empty: '{dst_object_name}'"
-        response = self.send_xml_request(dst_object_name, {"uploads": None}, "POST")
-        tree = ElementTree.fromstring(response.content)
-        upload_id = tree[2].text
-        return upload_id
+    def initiate_multipart_uploads(self, dst_object_names: List[str]):
+        upload_ids = []
+        for dst_object_name in dst_object_names:
+            assert len(dst_object_name) > 0, f"Destination object name must be non-empty: '{dst_object_name}'"
+            response = self.send_xml_request(dst_object_name, {"uploads": None}, "POST")
+            tree = ElementTree.fromstring(response.content)[2].text
+            upload_ids.append(tree)
+        return upload_ids
 
     def complete_multipart_upload(self, dst_object_name, upload_id):
         # get parts
diff --git a/skyplane/obj_store/object_store_interface.py b/skyplane/obj_store/object_store_interface.py
index c069028..6a52b95 100644
--- a/skyplane/obj_store/object_store_interface.py
+++ b/skyplane/obj_store/object_store_interface.py
@@ -84,7 +84,7 @@ class ObjectStoreInterface:
     def delete_objects(self, keys: List[str]):
         raise NotImplementedError()
 
-    def initiate_multipart_upload(self, dst_object_name):
+    def initiate_multipart_uploads(self, dst_object_names: List[str]):
         return ValueError("Multipart uploads not supported")
 
     def complete_multipart_upload(self, dst_object_name, upload_id):
diff --git a/skyplane/obj_store/s3_interface.py b/skyplane/obj_store/s3_interface.py
index 4d7ef47..2cd64fe 100644
--- a/skyplane/obj_store/s3_interface.py
+++ b/skyplane/obj_store/s3_interface.py
@@ -22,6 +22,7 @@ class S3Interface(ObjectStoreInterface):
         self.auth = AWSAuthentication()
         self.requester_pays = False
         self.bucket_name = bucket_name
+        self._cached_s3_clients = {}
 
     def path(self):
         return f"s3://{self.bucket_name}"
@@ -35,7 +36,7 @@ class S3Interface(ObjectStoreInterface):
             return region if region is not None else "us-east-1"
         except Exception as e:
             if "An error occurred (AccessDenied) when calling the GetBucketLocation operation" in str(e):
-                logger.error(f"Bucket location {self.bucket_name} is not public. Assuming region is us-east-1.")
+                logger.warning(f"Bucket location {self.bucket_name} is not public. Assuming region is us-east-1.")
                 return "us-east-1"
             logger.warning(f"Specified bucket {self.bucket_name} does not exist, got AWS error: {e}")
             raise exceptions.MissingBucketException(f"S3 bucket {self.bucket_name} does not exist") from e
@@ -48,7 +49,9 @@ class S3Interface(ObjectStoreInterface):
 
     def _s3_client(self, region=None):
         region = region if region is not None else self.aws_region
-        return self.auth.get_boto3_client("s3", region)
+        if region not in self._cached_s3_clients:
+            self._cached_s3_clients[region] = self.auth.get_boto3_client("s3", region)
+        return self._cached_s3_clients[region]
 
     @imports.inject("botocore.exceptions", pip_extra="aws")
     def bucket_exists(botocore_exceptions, self):
@@ -117,7 +120,7 @@ class S3Interface(ObjectStoreInterface):
         size_bytes=None,
         write_at_offset=False,
         generate_md5=False,
-        write_block_size=2**16,
+        write_block_size=2 ** 16,
     ) -> Optional[bytes]:
         src_object_name, dst_file_path = str(src_object_name), str(dst_file_path)
 
@@ -177,19 +180,20 @@ class S3Interface(ObjectStoreInterface):
                 raise exceptions.ChecksumMismatchException(f"Checksum mismatch for object {dst_object_name}") from e
             raise
 
-    def initiate_multipart_upload(self, dst_object_name):
-        assert len(dst_object_name) > 0, f"Destination object name must be non-empty: '{dst_object_name}'"
-        s3_client = self._s3_client()
-        with Timer(f"Initiate multipart upload for {dst_object_name}"):
-            response = s3_client.create_multipart_upload(
-                Bucket=self.bucket_name,
-                Key=dst_object_name,
-            )
-        return response["UploadId"]
+    def initiate_multipart_uploads(self, dst_object_names: List[str]) -> List[str]:
+        client = self._s3_client()
+        upload_ids = []
+        for dst_object_name in dst_object_names:
+            assert len(dst_object_name) > 0, f"Destination object name must be non-empty: '{dst_object_name}'"
+            response = client.create_multipart_upload(Bucket=self.bucket_name, Key=dst_object_name)
+            if "UploadId" in response:
+                upload_ids.append(response["UploadId"])
+            else:
+                raise exceptions.SkyplaneException(f"Failed to initiate multipart upload for {dst_object_name}: {response}")
+        return upload_ids
 
     def complete_multipart_upload(self, dst_object_name, upload_id):
         s3_client = self._s3_client()
-
         all_parts = []
         while True:
             response = s3_client.list_parts(
@@ -201,8 +205,6 @@ class S3Interface(ObjectStoreInterface):
                 if len(response["Parts"]) == 0:
                     break
                 all_parts += response["Parts"]
-
-        # sort by part-number
         all_parts = sorted(all_parts, key=lambda d: d["PartNumber"])
         response = s3_client.complete_multipart_upload(
             UploadId=upload_id,
diff --git a/skyplane/replicate/replicator_client.py b/skyplane/replicate/replicator_client.py
index 0e7ce73..b1bb490 100644
--- a/skyplane/replicate/replicator_client.py
+++ b/skyplane/replicate/replicator_client.py
@@ -1,3 +1,4 @@
+import itertools
 import json
 import math
 import pickle
@@ -384,11 +385,21 @@ class ReplicatorClient:
                     idx += 1
 
             # initiate multipart transfers in parallel
-            progress.update(prepare_task, description=f": Queuing {len(multipart_pairs)} files for multipart transfers")
+            progress.update(prepare_task, description=f": Initiating multipart transfers for {len(multipart_pairs)} objects")
             obj_store_interface = ObjectStoreInterface.create(job.dest_region, job.dest_bucket)
-            upload_ids = do_parallel(lambda x: obj_store_interface.initiate_multipart_upload(x[1].key), multipart_pairs, n=16)
+            with Timer("initiate_multipart_transfers"):
+                batch_size = max(1, len(multipart_pairs) // 64)
+                multipart_batches = []
+                for i in range(0, len(multipart_pairs), batch_size):
+                    multipart_batches.append(multipart_pairs[i : i + batch_size])
+                dispatch_fn = lambda x: obj_store_interface.initiate_multipart_uploads([y.key for _, y in x])
+                upload_ids = do_parallel(dispatch_fn, multipart_batches, n=-1)
+
+            # build chunks for multipart transfers
+            upload_ids = zip(
+                itertools.chain.from_iterable(i for i, _ in upload_ids), itertools.chain.from_iterable(o for _, o in upload_ids)
+            )
             for (src_object, dest_object), upload_id in upload_ids:
-                # determine number of chunks via the following algorithm:
                 chunk_size_bytes = int(multipart_chunk_size_mb * MB)
                 num_chunks = math.ceil(src_object.size / chunk_size_bytes)
                 if num_chunks > multipart_max_chunks:
@@ -419,13 +430,7 @@ class ReplicatorClient:
                     offset += chunk_size_bytes
                 # add multipart upload request
                 self.multipart_upload_requests.append(
-                    {
-                        "region": job.dest_region,
-                        "bucket": job.dest_bucket,
-                        "upload_id": upload_id,
-                        "key": dest_object.key,
-                        "parts": parts,
-                    }
+                    {"region": job.dest_region, "bucket": job.dest_bucket, "upload_id": upload_id, "key": dest_object.key, "parts": parts}
                 )
 
             # partition chunks into roughly equal-sized batches (by bytes)
@@ -579,11 +584,7 @@ class ReplicatorClient:
                             copy_gateway_logs = True
                             write_profile = True
                             write_socket_profile = True
-                            return TransferStats(
-                                monitor_status="error",
-                                total_runtime_s=t.elapsed,
-                                errors=errors,
-                            )
+                            return TransferStats(monitor_status="error", total_runtime_s=t.elapsed, errors=errors)
 
                         log_df = self.get_chunk_status_log_df()
                         if log_df.empty:
@@ -617,33 +618,33 @@ class ReplicatorClient:
                         )
                         if len(completed_chunk_ids) == len(job.chunk_requests):
                             if multipart:
-                                # Complete multi-part uploads
-                                def complete_upload(req):
-                                    obj_store_interface = ObjectStoreInterface.create(req["region"], req["bucket"])
-                                    succ = obj_store_interface.complete_multipart_upload(req["key"], req["upload_id"])
-                                    if not succ:
-                                        raise ValueError(f"Failed to complete upload {req['upload_id']}")
-
-                                do_parallel(
-                                    complete_upload,
-                                    self.multipart_upload_requests,
-                                    n=-1,
-                                    desc="Completing multipart uploads",
-                                    spinner=False,
-                                )
+                                # complete multipart transfers in batches
+                                progress.update(copy_task, description=" (completing multi-part uploads)")
+                                groups = {}
+                                for req in self.multipart_upload_requests:
+                                    key = (req["region"], req["bucket"])
+                                    if key not in groups:
+                                        groups[key] = []
+                                    groups[key].append(req)
+                                for key in groups:
+                                    with Timer(f"Complete multi-part uploads for {key[0]} {key[1]}"):
+                                        region, bucket = key
+                                        obj_store_interface = ObjectStoreInterface.create(region, bucket)
+                                        complete_fn = lambda req: obj_store_interface.complete_multipart_upload(
+                                            req["key"], req["upload_id"]
+                                        )
+                                        batch_len = max(1, len(groups[key]) // 128)
+                                        batches = [groups[key][i : i + batch_len] for i in range(0, len(groups[key]), batch_len)]
+                                        do_parallel(lambda x: map(complete_fn, x), batches, n=-1)
                             return TransferStats(
-                                monitor_status="completed",
-                                total_runtime_s=total_runtime_s,
-                                throughput_gbits=throughput_gbits,
+                                monitor_status="completed", total_runtime_s=total_runtime_s, throughput_gbits=throughput_gbits
                             )
                         elif time_limit_seconds is not None and t.elapsed > time_limit_seconds or t.elapsed > 600 and completed_bytes == 0:
                             logger.error("Transfer timed out without progress, please check the debug log!")
                             logger.fs.error("Transfer timed out! Please retry.")
                             logger.error(f"Please share debug logs from: {self.transfer_dir}")
                             return TransferStats(
-                                monitor_status="timed_out",
-                                total_runtime_s=total_runtime_s,
-                                throughput_gbits=throughput_gbits,
+                                monitor_status="timed_out", total_runtime_s=total_runtime_s, throughput_gbits=throughput_gbits
                             )
                         else:
                             current_time = datetime.now()
diff --git a/tests/interface_util.py b/tests/interface_util.py
index 9c4a13d..e01f3bf 100644
--- a/tests/interface_util.py
+++ b/tests/interface_util.py
@@ -24,7 +24,7 @@ def interface_test_framework(region, bucket, multipart: bool, test_delete_bucket
             file_md5 = hashlib.md5(f.read()).hexdigest()
 
         if multipart:
-            upload_id = interface.initiate_multipart_upload(obj_name)
+            upload_id = interface.initiate_multipart_uploads([obj_name])[0]
             time.sleep(5)
             interface.upload_object(fpath, obj_name, 1, upload_id)
             time.sleep(5)
