{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skyplane API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users are starting to compose Skyplane into larger applications! Data transfer across the cloud is often a critical component of most project pipelines, and Skyplane's simplicity and efficiency is an attractive tool for developers to handle this stage of the process. For easy and fast integration, Skyplane offers an API which enables users the same functionality as the CLI (e.g. copy, sync, etc.) along with some API-specific features. This has exciting implications for the growth of Skyplane and applications leveraging it going forward.\n",
    "\n",
    "Examples of use cases include but are not limited to: \n",
    "1. ML training, You will hear about Skypilot; \n",
    "2. Persistent synchronization, You can have incremental syncs to enable disaster recovery for apps running in other clouds; \n",
    "3. Skyplane Storage, Building storage API on top of the current API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skyplaneapi.png](./skyplaneapi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skyplane.api.api_class import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = SkyplaneClient()\n",
    "client.copy(src=\"s3://jason-us-east-1/fake_imagenet/\", dst=\"s3://jason-us-west-2/fake_imagenet_1/\", \n",
    "                   recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = client.new_session(\n",
    "    src_region=\"aws:us-east-1\", \n",
    "    dst_region=\"aws:us-west-2\", \n",
    "    num_vms=1, \n",
    "    solver=None # Skypane DirectSolver\n",
    ")\n",
    "\n",
    "with session as s:\n",
    "    session.auto_terminate()\n",
    "    s.copy(\"jason-us-east-1/fake_imagenet\", \"jason-us-west-2/fake_imagenet_test\", recursive=True)\n",
    "    s.copy(\"skyplane-us-east-1/video\", \"skyplane-us-west-2/video\", recursive=True)\n",
    "    # This will start the transfer in separate process\n",
    "    future1 = s.run_async()\n",
    "    s.copy(\"skyplane-us-east-1/texts\", \"skyplane-us-west-2/texts\", recursive=True)\n",
    "    # This will start the transfer in separate process\n",
    "    future2 = s.run_async()\n",
    "    # This waits the transfer until complete\n",
    "    s.wait_for_completion(future2)\n",
    "    s.wait_for_completion(future1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Training Leveraging Skyplane Data Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from awsio.python.lib.io.s3.s3dataset import S3IterableDataset\n",
    "\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import io\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetS3(IterableDataset):\n",
    "    def __init__(self, url_list, shuffle_urls=False, transform=None):\n",
    "        self.s3_iter_dataset = S3IterableDataset(url_list,\n",
    "                                                 shuffle_urls)\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def data_generator(self):\n",
    "        try:\n",
    "            while True:\n",
    "                # Based on aplhabetical order of files sequence of label and image will change.\n",
    "                # e.g. for files 0186304.cls 0186304.jpg, 0186304.cls will be fetched first\n",
    "                label_fname, label_fobj = next(self.s3_iter_dataset_iterator)\n",
    "                image_fname, image_fobj = next(self.s3_iter_dataset_iterator)\n",
    "                label = int(label_fobj)\n",
    "                image_np = Image.open(io.BytesIO(image_fobj)).convert('RGB')\n",
    "\n",
    "                # Apply torch visioin transforms if provided\n",
    "                if self.transform is not None:\n",
    "                    image_np = self.transform(image_np)\n",
    "                yield image_np, label\n",
    "\n",
    "        except StopIteration:\n",
    "            return\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.s3_iter_dataset_iterator = iter(self.s3_iter_dataset)\n",
    "        return self.data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "        print(f\"Current loss for batch#{i} is:\")\n",
    "        print(loss.item())\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = [\"s3://jason-us-east-1/imagenet-train-000000.tar\"]\n",
    "# Now if we want to transfer our dataset from us-east-1 to us-west-2 \n",
    "# since our vm is launched there\n",
    "# We can use Skyplane!!!\n",
    "client1 = SkyplaneClient()\n",
    "client1.copy(src=\"s3://jason-us-east-1/imagenet-train-000000.tar\", \n",
    "            dst=\"s3://jason-us-west-2/imagenet-train-000000.tar\", recursive=False)\n",
    "data_url = [\"s3://jason-us-west-2/imagenet-train-000000.tar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# Torchvision transforms to apply on data\n",
    "preproc = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "dataset = ImageNetS3(data_url, transform=preproc)\n",
    "\n",
    "train_loader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=2)\n",
    "\n",
    "model = models.__dict__['resnet18'](pretrained=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), 0.1,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can back up the model to the cloud, by Skyplane\n",
    "torch.save(model.state_dict(), \"./sample_model.pkl\")\n",
    "client1 = SkyplaneClient()\n",
    "client1.copy(src=\"./sample_model.pkl\", \n",
    "            dst=\"s3://jason-us-west-2/\", recursive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration into Existing Application Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-cloud jobs on Apache Airflow are common in data engineering pipelines, and the integration of Skyplane makes this process much faster. This is an ongoing collaboration with Max Demoulin at Astronomer, and we can see a very simple example of this integration below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code citation: GCS to S3 operator on Apache Airflow\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import TYPE_CHECKING, Sequence, List, Tuple\n",
    "\n",
    "from airflow.models import BaseOperator\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from airflow.utils.context import Context\n",
    "\n",
    "class SkyplaneOperator(BaseOperator):\n",
    "    template_fields: Sequence[str] = (\n",
    "        'src_bucket',\n",
    "        'src_region',\n",
    "        'dst_bucket',\n",
    "        'dst_region',\n",
    "        'config_path',\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *\n",
    "        src_region: str,\n",
    "        dst_region: str,\n",
    "        transfer_pairs: List[Tuple[str, str]],\n",
    "        config_path: str,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.src_region = src_region\n",
    "        self.dst_region = dst_region\n",
    "        self.transfer_pairs = transfer_pairs\n",
    "        self.config_path = config_path\n",
    "        \n",
    "    def execute(self, context: Context):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e615813c5489b1590d5f8b2d596a39a5f3baf5ccbb6dde5ecf5546914cc6cb8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
